### 圖片相關套件:NetworkX


## 使用套件

## scipy:

### integrality中的c必須是整數
![2](https://hackmd.io/_uploads/S17daiwe0.png)


## sympy-符號計算:

### sin(x)*exp(x)先做一次微分再來做二次微分
![3](https://hackmd.io/_uploads/Bya4Aiwg0.png)

## autograd:

### 不論是向量、單純數值、矩陣都可用tensor表示
###  requires_grad=True 該欄位要有梯度
### o.backward() 呼叫
### 最後x2+y2等於梯度，單一個等於偏微分
![4](https://hackmd.io/_uploads/ryfX-2ve0.png)


### [1.0,2.0] ->向量
### norm表示所有元素的平方和的開根號->歐氏距離
### 計算過程
![6](https://hackmd.io/_uploads/rJpCE2vgC.png)
![7](https://hackmd.io/_uploads/B1WJr3weA.png)

![5](https://hackmd.io/_uploads/ryCAfhPgA.png)



## torchGd:


### learning_rate學習率 、loop_max最大迴圈數
### loss = loss_fn(x) 呼叫損失函數
### 為了讓損失降低，使用loss.backward()算出梯度
###  torch.no_grad() 梯度下降
### x -= learning_rate * x.grad 對每個參數往逆梯度方向前進，前進的距離要看learning_rate，learning_rate為1e-3為0.001

![9](https://hackmd.io/_uploads/HJ9P8nPxA.png)


## Cross-entropy:
### 樣本是以統計
### 機率是已知實體、來源分布
![10](https://hackmd.io/_uploads/r1-aunveR.png)
### p(x)為真、q(x)為假
### 以機率為範例:
### 2位元*2、1位元*1
### 前者Pi指出現機率，後者的log Pi，log是取2為底，Pi是編碼位長度是先預設a -> 11 、b -> 10 、 c -> 0
### 最後平均位元數的機率為二分之三，表示最小(最佳編碼方式)
### Cross-entropy的目的為越小越好，透過樣本學習等，最終希望讓Pi=Qi，並找到實際是分布多少
![437803853_3556650621262849_2491719683633727906_n](https://hackmd.io/_uploads/Bkixw6veC.jpg)


### self為一個向量[1/2,1/4]，並取log(log_probs)，yb(標準答案)乘以log_probs等於zb(Plog(q))
![11](https://hackmd.io/_uploads/HkRSmTPlR.png)

### 編碼上最佳化可當作ai的目標函數

### Pytorch損失函數
https://pytorch.org/docs/stable/nn.html

### 不偏估計:估計量的期望值等於真實參數值

