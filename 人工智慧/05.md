### 深度學習=神經網路, 差別在於深度學習多了模型


## 梯度
### 對每個變數用偏微分(偏導數)一次，且把那些已經偏微分過的變數蒐集起來
### 各方向向量的集合
![1](https://hackmd.io/_uploads/BkfBXncAT.png)

## 梯度下降:
### 如果梯度計算出來不是0，就會往逆梯度方向前進(-1*h)，如果往正梯度走會不斷上升。
![2](https://hackmd.io/_uploads/HkwQOh50p.png)


## 正傳遞:
![42](https://hackmd.io/_uploads/B1ONIq4Bp.png)

## 反傳遞:
### q的偏微分=z，z的偏微分=q，已知f=1，要算出q的偏微分就是z，往回乘-->q=1*-4=-4，z就不需要算偏微分了，因為已經是輸入值(樣本)，z=3，神經網路只能調整中間的權重，不能調整輸入值，剩下計算內容請看下圖:
![43](https://hackmd.io/_uploads/B1mRic4r6.png)

## 反傳遞算法:
### data-->正傳遞的計算值(q=x+y)、grad-->梯度，prev前面網路節點的部分是例如:q的prev是x、y,f的prev是q、z，init是建構函數(前後都加底線)
![48](https://hackmd.io/_uploads/rylM1iVHT.png)
```
f(x,y,z)=(x+y)*z
```
### 反傳遞算法:
![3](https://hackmd.io/_uploads/HygHF35R6.png)
