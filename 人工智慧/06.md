### 線性 -> 符合向量加法乘法或是兩個變數之間的變化是直線型態的
### 範例:y = 2x + 3

### 非線性 -> 非直線，二次曲線
### 範例: y = x^2

### 參考
https://medium.com/@adea820616/activation-functions-sigmoid-relu-tahn-20e3ae726ae


## ReLu:
### 會導致梯度變0，梯度下降失效，神經網路收斂
![12](https://hackmd.io/_uploads/ByLnQAPeC.png)

## Leaky ReLu(解決Relu的缺點):
### 改善後可繼續執行
![13](https://hackmd.io/_uploads/rkmwNRvl0.png)


## micrograd:
### f = x**n,微分等於nx^n-1次方*反傳遞值
![1](https://hackmd.io/_uploads/rycvqQ5lR.png)

## macrograd:

### 單一值 -> value、scalar(純量)
### 多維 -> tensor(張量)


### 在shape中，向量、常數、矩陣甚至是三維(多矩陣)都可作為一個張量
### tensor代表n維陣列
![2](https://hackmd.io/_uploads/ryw1hQ9xA.png)

### 乘法不是內積，原理是原數與原數相乘
![螢幕擷取畫面 2024-04-15 122911](https://hackmd.io/_uploads/r1vdaXqgR.png)


### 矩陣相乘的反傳遞
### 反傳遞 -> 向量反傳遞回來
### self.data.T是指data矩陣的transforms相乘反傳遞out.grad
![437947946_2394555880875468_5979634973217162891_n](https://hackmd.io/_uploads/H14I-N5eA.jpg)

![4](https://hackmd.io/_uploads/rkwlCQ5lA.png)

### 參考
https://github.com/cccbook/py2gpt/wiki/softmax
https://mattpetersen.github.io/softmax-with-cross-entropy

### 從這個公式可以看到把每個(X)i輸入後取e^x次方，考慮到有n個，因此全部加總當分母，分子為加總的其中一個
![6](https://hackmd.io/_uploads/ByAMVE5eA.png)
### 答案為s - y ，其中的 s = softmax(x)
![5](https://hackmd.io/_uploads/ryzkX4cxC.png)


## autoGrad:

### sine用數值的方法計算sine函數
![7](https://hackmd.io/_uploads/HJZdDVcgC.png)

### 線性回歸 -> 尋找一條線去做擬合
### 採用邏輯回歸採用加上sigmoid函數，建立dataset開始訓練，並做梯度下降
![8](https://hackmd.io/_uploads/BkD4Y4cgR.png)


## AutoEncoder:
### 一個Encoder，輸出一個vector，其中這個輸出的vector又可以稱為Laten Representation、Latent Code，而Decoder的部份則是以Encoder的output做為input，試圖還原輸入的資訊。訓練的時候，我們要讓Encoder的輸入與Decoder的輸出愈接近愈好。

### 這種模型由兩部分組成：編碼器(encoder)和解碼器(decoder)。編碼器負責將輸入資料壓縮成低維表示，而解碼器則負責將這個低維表示解碼成原始輸入資料。
![9](https://hackmd.io/_uploads/B1kt9V5x0.png)


## softmax:
### x與y的誤差就是梯度
### 計算交叉熵損失函數對 softmax 輸出的梯度時，結果通常是 softmax 輸出和目標值之間的差異。這種差異代表了模型預測和實際目標之間的誤差。
### 如果我們用 s-y，我們得到了一個向量，其中每個元素表示對應類別的預測值和目標值之間的差異。換句話說，這個向量代表了模型的誤差。
![10](https://hackmd.io/_uploads/Bk4eJB5gR.png)


## CrossEntropy(交叉熵):
### 觀測預測的機率分佈與實際機率分布的誤差範圍
### 舉個例子:我們預測的機率分佈為橘色區塊，真實的機率分佈為紅色區塊，藍色的地方就是 cross-entropy 區塊，紫色現為計算出來的值。
### 前面提到，預測值與實際值差越多，也就是代表內涵的資訊量愈大，也就是不確定越多，也就是 Cross-entropy 會越高。
![11](https://hackmd.io/_uploads/SJsTbLI26.png)

### 反之，如果疊合的區塊越多，就代表不確定性越少，也就是 cross-entropy 會越小
![12](https://hackmd.io/_uploads/BkB1M8Lhp.png)
