## 馬可夫鏈
### 一種隨機過程，它具有馬可夫性質，即當前狀態的未來發展只與當前狀態有關，與過去狀態無關。一個馬可夫鏈可以用一個有限或無限的狀態集合來表示，並且每個狀態都有一個轉移概率矩陣，表示在當前狀態下轉移到下一個狀態的概率。



## 馬可夫鏈-迭代學習機率:
### 初斂平衡- 每個節點輸出輸入的機率是一樣
### 細緻平衡- 每條線輸出輸入的機率是一樣



## 狀態機率平衡－gibbs算法+蒙地卡羅法:
### gibbs算法- 在已知「轉移矩陣」的情況下求解每個狀態的平衡機率值
### 蒙地卡羅法-隨機模擬出最後結果，一種機率式狀態機，與狀態機不同的是有機率，轉移是機率式的狀態

## 序列式的機率
### 一開始b的機率為0.8，b轉移到a的機率 -> b=>a = 0.5，a轉移到b -> 0.3，b轉移到b -> 0.5，全部相乘 -> 0.06

![1](https://hackmd.io/_uploads/H1F9RTxMR.png)

## 平衡機率:
### 底下結果來看，第一輪->a:0.2*0.7 + 0.8*0.5 =0.54 、b:0.2*0.3 + 0.8*0.5=0.46
### 第二輪-> a:0.54*0.7 + 0.46 0.5 =0.608 、b:0.54*0.3+0.46*0.5=0.392
### 接著就以此類推，da代表(下一輪機率)a-(上一輪機率)a，db代表(下一輪機率)b-(上一輪機率)b，step代表da*da+db*db的平方根，差異小於0.001就停止，最後印出
![3](https://hackmd.io/_uploads/SJH0QAxf0.png)


## 在蒙地卡羅法上執行gibbs算法:
### MCMC 的 Gibbs 抽樣方法，通過一定數量的迭代（這裡是 1000000 次）來模擬系統的狀態，並更新機率 P，以便最終收斂到一個平衡分佈
![4](https://hackmd.io/_uploads/rknA_ReGC.png)

## 馬可夫決策過程:
### 其優化目標是為決策者找到一個好的策略
### 舉例:假設你是一個機器人，被派往一個倉庫進行自動化運送，你需要從倉庫的起點將商品送到不同的目的地，然後返回起點。每個目的地都有不同的距離和交通情況，並且完成每次運送都會獲得一定的報酬。然而，你也需要考慮到移動的成本，例如能量消耗和時間成本。為了達到這個目標，你可以使用各種策略，例如選擇移動到最近目的地的位置，或者優先選擇最有可能通過的路徑。通過分析每個可能的行動和狀態轉移，你可以找到一個最優的策略。

## 貝爾曼方程:
### 簡化強化學習或者馬爾可夫决策問題
### 舉例馬爾可夫獎勵過程的價值函数（Value function）的定義，分為即時獎勵Rt+1、加了權重y的後續狀態的價值函數yu(St+1)
### 以下是其數學表達式:
```
v(s)=E[Rt+1+yv(St+1)| St-s]
```
### 對於貝爾曼方程只往前考慮一步。在某個狀態下，我做了一個行動，得到了立即回報。我就可以將這個立即回報加上未來後續狀態的價值函數做為我的總體回報。也就是上式中的Rt+1+yv(St+1)





## Q-Learning 算法:
### 无模型（model-free）的强化学习算法，它通过迭代地更新动作值函数来逐渐逼近最优动作值函数，从而实现最优的决策。

### 公式:`$$Q(s_t, a_t)←Q(s_t, a_t)+\alpha(reward+\gamma \max_{a} Q(s_{t+1},a)-Q(s_t,a_t))$$`
### $Q(s_t, a_t)表示在特定狀態st下採取特定動作at所得到的預期收益值。
### alpha是指學習效率，控制了每次更新中新資訊對於原有估計的影響程度。
### reward 是在狀態st下執行動作at後所獲得的即時獎勵。
### gamma是折扣因子，用於衡量未來獎勵的價值。
### max_{a} Q(s_{t+1},a)表示在狀態st+1中，採取所有可能動作後的最大預期收益值。

