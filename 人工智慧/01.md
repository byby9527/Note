## 一維函數微分
![1](https://hackmd.io/_uploads/SJffnCS3p.png)

## 梯度
### 此圖可以看到這是三維的曲面代表是二維的函數，高度為函數值，輸入:x、y，高度:f(x,y)，箭頭所表示上方所對應的點梯度為多少，梯度為最斜方向的斜率，因此給予任意一點都會計算出箭頭方向，此稱為梯度。
![2](https://hackmd.io/_uploads/HJhNaRH26.png)

 ## 偏微分
 ### 把某一特定的當成變數，其餘當成常數，x*x對應x微分->2x，xy對應x微分-->y(常數)，y*y對應x微分->把y視為常數，因此為0
![3](https://hackmd.io/_uploads/ByLW-183T.png)

### 梯度算法:執行迴圈把所有偏微分排成一排，最後得到的向量為梯度
![4](https://hackmd.io/_uploads/ryc7zy8hp.png)

![5](https://hackmd.io/_uploads/H1F_NyL3a.png)

## 向量:空間的距離到零之間

## Loss function(區域最低點):損失函數中的損失就是實際值和預測值的殘差。

## CrossEntropy(交叉熵):
### 觀測預測的機率分佈與實際機率分布的誤差範圍
### 舉個例子:我們預測的機率分佈為橘色區塊，真實的機率分佈為紅色區塊，藍色的地方就是 cross-entropy 區塊，紫色現為計算出來的值。
### 前面提到，預測值與實際值差越多，也就是代表內涵的資訊量愈大，也就是不確定越多，也就是 Cross-entropy 會越高。
![11](https://hackmd.io/_uploads/SJsTbLI26.png)

### 反之，如果疊合的區塊越多，就代表不確定性越少，也就是 cross-entropy 會越小
![12](https://hackmd.io/_uploads/BkB1M8Lhp.png)



## 梯度下降法:



### 給予隨機三變數，透過梯度下降法就會找到三變數的最低點
### 最低點為x=1，y=2,z=3
![8](https://hackmd.io/_uploads/r1gdRS82a.png)
![9](https://hackmd.io/_uploads/r1D4kLU2a.png)


### 尋找損失函數，在MSE中，在5個點上(y)-線性方程上的線所預測x、y值為多少，最後得到的誤差再乘以平方加總表示為最小平方法
### 損失函數=MSE
### 使用梯度下降找出預測損失率的最小值且最後繪畫出結果圖
![6](https://hackmd.io/_uploads/ryhHHk82p.png)

### 梯度下降法顯示了尋找5個點並在同一條線上擬合，被稱為回歸
![7](https://hackmd.io/_uploads/ByPISkInp.png)


### SOTA:State of the ART(先進科技)
