## 人工智慧:
### 優化法-爬山演算法
### 搜尋法-深度優先搜尋DFS


## 神經網路:
### momentum(動量)可以解決梯度下降變成0就無法執行

## 隨機梯度下降（SGD）:
### 學習率太大，容易造成參數更新呈現鋸齒狀的更新，這是很沒有效率的路徑。
### 需考慮動量、步伐大小

## dropout:
### 停止運作某些神經，就像是從神經網路將這些神經去除一樣
### Dropout 為什麼可以解決 Overfitting
### 造成Overfitting: Network 過於複雜（參數量很多）但是擁有的訓練資料又很少



### Sigmoid函数是理解神經網路如何學習複雜問題

## ReLU:
### 一種類神經網路中活化函數 (activation function)的一種
### 用來增加類神經網路模型的非線性，讓我們定義的類神經網路可以更加活化學習，避免像是線性函數一樣較為死板。

### kernel size -> 卷積的矩陣
### in_channel 代表輸入的feature 層數
### out_channel代表輸出的feature 層數

### stride 控制圖像長寬尺寸的方式，會影響Feature map大小
