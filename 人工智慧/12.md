![1](https://hackmd.io/_uploads/Hka05-5zA.png)

## Type0語言 - 遞歸可枚舉語言 (Recursive Enumerable , RE):
### 所有語言的集合
### 枚舉器是圖靈機的一種變種，不需要接受输入，枚舉器E所打印出的字串的集合稱為該枚舉器的語言。
### 透過一個特定的枚舉器（記為E）逐步列舉出的語言集合。這個枚舉器會列印出所有屬於該語言集合的字串。如果一個語言集合S可以由某個枚舉器E完全列舉出來，那麼稱這個語言集合S是遞歸可枚舉的。
### 範例:有一个程式E，它能夠打印出所有的偶數，那么所有的偶數的集合就是遞歸可枚舉的，因为我们可以通過這個程式逐步列舉出所有的偶數。




## Type1 語言- 對上下文敏感的語言 (Context-Sensitive):
### 每個規則的左邊至少要有一個非終端項目 A，但其前後可以連接任意規則
### 前後文之間是有關係的
### 範例-生成aaabbbccc 的案例：
![2](https://hackmd.io/_uploads/Sk2djb5MC.png)

## Type 2 語言-上下文無關的語言(Context Free):
### 規則左邊只能有一個非終端項目 (以 A 代表)，規則右邊則沒有限制
### 用自然語言表示:
![3](https://hackmd.io/_uploads/S14cab5fA.png)

## Type 3語言- 正規式(Regular):
### 規則的左右兩邊都最多只能有一個非終端項目 (以 A, B 表示) ，而且右端的終端項目 (以 a 表示) 只能放在非終端項目 B 的前面
### 舉例-正規表達式表示ab*，*表示前一個字符的零次或多次重複
### 結果能以'a', 'ab', 'abb', 'abbb', 等等


## 格狀語法 (Case Grammar):
### 不需要先進行剖析，程式會跳過剖析的階段，直接採用物件導向式的方式，會自行根據語意規 則比對的方式進行理解
### 不像是一種語法，而是一種圖形表達結構，透過`欄位填充機制`可將詞彙填入到這些格子
### 根據理解後所得出的結果為:爸爸用榔頭敲釘子
![5](https://hackmd.io/_uploads/BJPXnZsf0.png)



### 參考:
https://github.com/cccresearch/ai2
## 大型語言模型(LLM) 
### 深度學習模型，透過吸收大量的文本數據去學習，能從大量的內容中之間的關係，然後回答問題、翻譯、生成文本等
### 無法處理需要深度思考的 System2 問題


## 範例-AI2 -- 呼叫 System2的LLM:
### 可以看到在輸出的時候，用 <python>code</python> 把那些需要 plugin 介入的部分標示出來。
### 而這些 plugin code 當中，對於 LLM 已經知道該呼叫那些函數去做的，由 Python 程式去處理，而不知道該由哪些函數去處理的，則是交給system2()函數去處理。

![6](https://hackmd.io/_uploads/r1P_MMsfC.png)


### 參考:
https://github.com/cccbook/py2gpt/wiki/rnn
## Rnn循環神經網路:
### 沒有循環神經網路 -> 組合邏輯 ，反之 -> 循序邏輯
### RNN 循環神經網路透過加入循環，讓 RNN 可以記住過去的資訊在隱藏層 h 中
![7](https://hackmd.io/_uploads/Syqo8MizA.png)

### 程式碼:
### `self.rnn`定義了一個循環神經網路層，根據method參數，它可以是一個RNN或GRU層，它接受嵌入向量作為輸入，並返回隱藏狀態。
### 其做法為 -> 將x丟給h，變成新的值，再丟給h，這邊表示為線性層，輸出為h，輸入x+h，但輸出跟輸入的h都是同一個，因此就變成循環了

```
class RNNLM(nn.Module):
    def __init__(self, method, vocab_size, embed_size, hidden_size, num_layers):
        super(RNNLM, self).__init__()
        method = method.upper()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.RNN(embed_size, hidden_size, num_layers, batch_first=True) # RNN 也可以改為 GRU
        self.linear = nn.Linear(hidden_size, vocab_size)
        
    def forward(self, x, h):
        # Embed word ids to vectors
        x = self.embed(x)
        
        # Forward propagate 
        out, h = self.rnn(x, h)
        
        # Reshape output to (batch_size*seq_length, hidden_size)
        out = out.reshape(out.size(0)*out.size(1), out.size(2))
        
        # Decode hidden states of all time steps
        out = self.linear(out)
        return out, h
```


### 這邊可以看到使用RNN或GRU去做循環，hiddens表示n層，做了t次循環
### `ht = self.cell(xt, hprev)`使用RNN或GRU計算`ht`(隱藏狀態)
### `hprev = ht`將當前計算得到的隱藏狀態ht更新為下一個的前一隱藏狀態hprev
![8](https://hackmd.io/_uploads/B1egyXjz0.png)



## 詞向量 word2vec:
### 用於將詞彙表示為連續向量空間中的點
### 模型分別為CBOW、Skip-Gram
### Skip-Gram的想法是用一個詞去預測該情況
### CBOW與Skip-Gram相反，使用目標字詞(中間字詞)來推測周圍字詞(上下文)


### NLP-讓電腦去解釋或理解人類使用的語言

